---

# This file is distributed under the Apache License (the
# "License"); you may not use this file except in compliance
# with the License.  See the LICENSE file distributed with 
# this work for additional information regarding copyright
# ownership. 
# 
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
# This file is derived from recipe.sh in OpenHPC distribution 
# and ported by Linaro Ltd. and Fujitsu Ltd.

#
#roles/slurm-client/tasks/main.yml
#

#yum -y --installroot=$CHROOT groupinstall ohpc-slurm-client
- name: Install ohpc-slurm-client for computing node images on master
  yum: name={{ item }} installroot="{{ compute_chroot_loc }}" state=latest
  with_items:
    - ohpc-slurm-client
  when:
    - inventory_hostname in groups[nt_sms]
    - enable_warewulf == true

- name: Install ohpc-slurm-client on computing nodes
  yum: name={{ item }} state=latest
  with_items:
    - ohpc-slurm-client
  when:
    - inventory_hostname in groups[nt_cnodes]
    - enable_warewulf == false

# Enable slurm pam module
#echo "account    required     pam_slurm.so" >> $CHROOT/etc/pam.d/sshd
- name: Enable slurm pam module for computing node images on master
  lineinfile:
    path: "{{ compute_chroot_loc }}/etc/pam.d/sshd"
    state: present
    backrefs: no
    regexp: '^account    required     pam_slurm\.so$'
    line: "account    required     pam_slurm.so"
  when:
    - inventory_hostname in groups[nt_sms] 
    - enable_warewulf == true

- name: Enable slurm pam module on a computing node
  lineinfile:
    path: "/etc/pam.d/sshd"
    state: present
    backrefs: no
    regexp: '^account    required     pam_slurm\.so$'
    line: "account    required     pam_slurm.so"
  when:
    - inventory_hostname in groups[nt_cnodes]
    - enable_warewulf == false

- block:
  - name: Identify resource manager hostname on master
    replace: dest=/etc/slurm/slurm.conf regexp='ControlMachine=\S+' replace="ControlMachine={{ sms_name }}" backup=yes

  - name: slum configuration on master and computing node    # Todo:It may not be written here
    replace: dest=/etc/slurm/slurm.conf regexp='^NodeName=(\S+).+State=(\S+)' replace="NodeName={{ compute_prefix }}[1-{{ num_computes }}] State=\2" backup=yes

  - name: slum configuration on master and computing node    # Todo:It may not be written here
    replace: dest=/etc/slurm/slurm.conf regexp='^PartitionName=normal Nodes=(\S+)' replace="PartitionName=normal Nodes={{ compute_prefix }}[1-{{ num_computes }}]" backup=yes

  - name: Change Max Time to infinite
    replace: dest=/etc/slurm/slurm.conf regexp='MaxTime=(\d+:\d+:\d+)' replace="MaxTime={{ slurm_max_time }}" backup=yes

  - name: FastSchedule to 0 so nodes report their architecture
    replace: dest=/etc/slurm/slurm.conf regexp='^FastSchedule=(\d+)' replace="FastSchedule=0" backup=yes

  - name: Set MpiDefault default is none/up to the implementation (fails on aarch64)
    replace: dest=/etc/slurm/slurm.conf regexp='^MpiDefault=(\S+)' replace="MpiDefault={{ slurm_mpi_default }}" backup=yes

  when: 
    ( ( inventory_hostname in groups[nt_sms] ) or
    ( inventory_hostname in groups[nt_devnodes] ) or
    ( inventory_hostname in groups[nt_cnodes] ) ) and ( enable_warewulf == false )

- block:
  - name: Identify resource manager hostname on master
    replace: dest="{{ compute_chroot_loc }}/etc/slurm/slurm.conf" regexp='ControlMachine=(\S+)' replace="ControlMachine={{ sms_name }}" backup=yes

  - name: slurm configuration on master    # Todo:It may not be written here
    replace: dest="{{ compute_chroot_loc }}/etc/slurm/slurm.conf" regexp='^NodeName=(\S+).+State=(\S+)' replace="NodeName={{ compute_prefix }}[1-{{ num_computes }}] State=\2" backup=yes

  - name: slurm configuration on master     # Todo:It may not be written here
    replace: dest="{{ compute_chroot_loc }}/etc/slurm/slurm.conf" regexp='^PartitionName=normal Nodes=(\S+)' replace="PartitionName=normal Nodes={{ compute_prefix }}[1-{{ num_computes }}]" backup=yes

  - name: Change Max Time to infinite
    replace: dest="{{ compute_chroot_loc }}/etc/slurm/slurm.conf" regexp='MaxTime=(\d+:\d+:\d+)' replace="MaxTime={{ slurm_max_time }}" backup=yes

  - name: FastSchedule to 0 so nodes report their architecture
    replace: dest="{{ compute_chroot_loc }}/etc/slurm/slurm.conf" regexp='^FastSchedule=(\d+)' replace="FastSchedule=0" backup=yes

  - name: Set MpiDefault default is none/up to the implementation (fails on aarch64)
    replace: dest="{{ compute_chroot_loc }}/etc/slurm/slurm.conf" regexp='^MpiDefault=(\S+)' replace="MpiDefault={{ slurm_mpi_default }}" backup=yes

  when: 
    - inventory_hostname in groups[nt_sms]
    - enable_warewulf == true
    - ansible_machine != "aarch64"

# Note: In typical cases we run this playbook on sms as normal user.
# We should change the file/directory mode before copying it.
#
- name: Change the file mode of /etc/munge/munge.key to copy on master
  file: path=/etc/munge/munge.key state=file mode=0644
  when:
    - inventory_hostname in groups[nt_sms]
    - enable_warewulf == false

- name: Change the file mode of /etc/munge to copy on master
  file: path=/etc/munge state=directory mode=0755
  when:
    - inventory_hostname in groups[nt_sms]
    - enable_warewulf == false

- name: Fetch /etc/munge/munge.key in sms to a ansible local /tmp
  fetch:
    src: /etc/munge/munge.key
    dest: "{{ role_path }}/munge.key"
    flat: yes
  when:
    - inventory_hostname in groups[nt_sms]
    - enable_warewulf == false

- name: Copy /etc/munge/munge.key in sms to a compuing node
  copy:
    src:  "{{ role_path }}/munge.key"
    dest: /etc/munge/munge.key
    owner: munge
    group: munge
    mode: 0400
  when:
    - inventory_hostname in groups[nt_cnodes]
    - enable_warewulf == false

- name: Remove /tmp/munge.key on ansible local
  delegate_to: localhost
  become: no
  file: path="{{ role_path }}/munge.key" state=absent
  when:
    - inventory_hostname in groups[nt_sms]
    - enable_warewulf == false

- name: Fix the file mode of /etc/munge/munge.key  on master
  file: path=/etc/munge/munge.key state=file mode=0400
  when:
    - inventory_hostname in groups[nt_sms]
    - enable_warewulf == false

- name: Fix the file mode of /etc/munge on master
  file: path=/etc/munge state=directory mode=0700
  when:
    - inventory_hostname in groups[nt_sms]
    - enable_warewulf == false

# Note: We need to make slurmd on computing node enabled.
# Even though it might be also needed for recipe.sh, but it has not been written in current recipe.sh.
#
- name: Enable a munge for computing images on master
  command: chroot "{{ compute_chroot_loc }}" systemctl enable munge
  register: result
  failed_when: result.rc not in [0]
  when:
    - inventory_hostname in groups[nt_sms]
    - enable_warewulf == true

- name: Enable a munge client on a computing node
  service: name=munge enabled=yes
  when:
    - inventory_hostname in groups[nt_cnodes]
    - enable_warewulf == false
    
- name: Enable a slurm client for computing images on master
  command: chroot "{{ compute_chroot_loc }}" systemctl enable slurmd
  register: result
  failed_when: result.rc not in [0]
  when:
    - inventory_hostname in groups[nt_sms]
    - enable_warewulf == true

- name: Enable a slurm client on a computing node
  service: name=slurmd enabled=yes
  when:
    - inventory_hostname in groups[nt_cnodes]
    - enable_warewulf == false

# ------------------------------------
# Resource Manager Startup (Section 5)
# ------------------------------------

#systemctl enable munge
- name: enable munge on master
  service: name=munge enabled=yes
  when:
    - inventory_hostname in groups[ nt_sms ]

#systemctl enable slurmctld
- name: enable slurmctld on master
  service: name=slurmctld enabled=yes
  when:
    - inventory_hostname in groups[ nt_sms ]

#systemctl start munge
- name: start munge on master
  service: name=munge state=started
  when:
    - inventory_hostname in groups[ nt_sms ]

#systemctl start slurmctld
- name: start slurmctld on master
  service: name=slurmctld state=started
  when:
    - inventory_hostname in groups[ nt_sms ]
